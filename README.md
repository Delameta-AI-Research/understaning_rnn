# Introduction
Achieving a research-level understanding of artificial intelligence topics in 2020 is like climbing a mountain. Several theories are based on vast bodies of subject such as information theory, probability theory, signal processing, linear algebra and several works on artificial intelligence itself. Some resource out there are very sparse, unstructured, and hard to understand. This repository is an attemp to explain the inner working of one of the popular AI topics that has show tremendous progress on both Computer Vision and Natural Language Processing (i.e RNN based neural network architecture) by laveraging the power of Jupyter Notebook. We will explain popular architecture of RNN such as LSTM, GRU, MGU, Attention Mechanism, Neural Turing Machine and language transformer by implementing the code from scratch using PyTorch, and deriving necessary mathematical intuition.    

## Table of Contents
1. [Introduction](https://github.com/Delameta-AI-Research/understaning_rnn)
2. Recurrent Neural Network<br> 
   a. Vanilla RNN and Backpropagation Through Time<br>
   b. LSTM<br>
   c. GRU<br>
   d. MGU<br>
3. Adved Recurrent Neural Network Architecture<br>
   a. Attention Mechanism<br>
   b. Neural Turing Machine<br>
